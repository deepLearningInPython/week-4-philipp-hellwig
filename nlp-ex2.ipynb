{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":763778,"sourceType":"datasetVersion","datasetId":362178},{"sourceId":112347694,"sourceType":"kernelVersion"}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This exercise is part of the [Token Embeddings](https://www.kaggle.com/datasniffer/nlp-token-embeddings) tutorial**\n\n---\n\n\n# Train your own vector embeddings\n\nBERT works with vector embeddings—numerical vectors that represents the contextualized meaning of words in a sequences. They are state of the art representations of semantics and evolved out of more simple text embeddings such as Word2Vec and GloVe.\n\nAs aluded to in the tutorial (tutorial 2 for this NLP course), there is an entire zoo out there of pre-trained word embeddings that you can download from the internet and use to your advantage; including the most widely known:\n\n- FastText (multiple dimensions available, for many languages)\n- GloVe (multiple dimensions available, for English)\n- Word2vec (multiple dimensions available, for multiple languages)\n\nBecause these word vectors have been trained on huge amounts of data, they are of very high quality. However, this also means that they may not contain words or tokens or symbols that are relevant to your task at hand. In those cases, you may very well benefit from creating your own word (token) embeddings, customized specifically for the data you have at hand.\n\nIn this exercise you will create your own word embeddings. But instead of using the approach demonstrated in the tutorial, we'll use a library called `gensim` which makes this a little easier and faster.\n\n\nParts of this exercise are based on [this Kaggle notebook](https://www.kaggle.com/code/chewzy/tutorial-how-to-train-your-custom-word-embedding). \n\nLet's get started. First, run the next code cell.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools_nlp_utility import *\n\nprint(\"\\nSetup complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:46:08.554846Z","iopub.execute_input":"2024-11-22T12:46:08.555711Z","iopub.status.idle":"2024-11-22T12:46:26.787587Z","shell.execute_reply.started":"2024-11-22T12:46:08.555609Z","shell.execute_reply":"2024-11-22T12:46:26.786218Z"}},"outputs":[{"name":"stdout","text":"\nSetup complete\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def cleanup_review(review):\n    import re\n    \n    # Mr., Mrs., Dr., etc.\n    review = re.sub(\"\\s(Dr|Mr|Mrs|Ms|vs|U.S)\\.\\s\", \" \\\\1_ \", review)\n    \n    # . is going to be a token\n    review = re.sub(\"\\s+\\.\\s|(?<=\\w)\\.(?=[A-Z\\W])|.$\", \" . \", review)\n    \n    # 's, 'd, 'll are going to be tokens\n    review = re.sub(\"'(s|d|ll|ve)\\s\", \" _\\\\1 \", review)\n    \n    # n't\n    review = re.sub(\"n't\\s\", \" not \", review)\n    \n    # .., ..., ...., etc. is going to be a token\n    review = re.sub(\"\\.{2,}\", \" ... \", review)\n    \n    # , is going to be a token\n    review = re.sub(\"(?<=\\w)([,!?])\", \" \\\\1 \", review)\n    \n    # \\W is going to be a token\n    # move text in parentheses to the back\n    while re.findall(\"\\s\\(([^)]+)\\)\", review):\n        review = re.sub(\"(.+?)\\s\\(([^)]+)\\)[.!?]?(.+)\", \"\\\\1 \\\\3 <br />\\\\2 \", review) \n        \n    # remove \" and '\n    review = re.sub(\"['\\\"()]\", \" \", review)\n    \n    # each white space is only one space\n    review = re.sub(\"\\s+\",\" \", review)\n    \n    return review\n\n# Read in the data\nnewdat = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# Clean up the reviews\nnewdat['review'] = newdat.review.map(lambda x: cleanup_review(x).lower())\nnewdat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:46:26.789387Z","iopub.execute_input":"2024-11-22T12:46:26.789743Z","iopub.status.idle":"2024-11-22T12:46:50.715984Z","shell.execute_reply.started":"2024-11-22T12:46:26.789710Z","shell.execute_reply":"2024-11-22T12:46:50.714908Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n0      one of the other reviewers has mentioned that ...  positive\n1      a wonderful little production . <br /><br />th...  positive\n2      i thought this was a wonderful way to spend ti...  positive\n3      basically there _s a family where a little boy...  negative\n4      petter mattei _s love in the time of money is ...  positive\n...                                                  ...       ...\n49995  i thought this movie did a down right good job...  positive\n49996  bad plot , bad dialogue , bad acting , idiotic...  negative\n49997  i am a catholic taught in parochial elementary...  negative\n49998  i m going to have to disagree with the previou...  negative\n49999  no one expects the star trek movies to be high...  negative\n\n[50000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a wonderful little production . &lt;br /&gt;&lt;br /&gt;th...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically there _s a family where a little boy...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei _s love in the time of money is ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>i thought this movie did a down right good job...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>bad plot , bad dialogue , bad acting , idiotic...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>i am a catholic taught in parochial elementary...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>i m going to have to disagree with the previou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>no one expects the star trek movies to be high...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Step 1: Splitting texts into word sequences\n\nAs we did in the tutorial we need to split the review texts up into sentences/fragments, and then the sentences into word sequences. Write the code to split the sentences into lists of words (using a space as the split) in the list `train_sentences` below:","metadata":{}},{"cell_type":"code","source":"from itertools import chain\n\n# The following splits reviews into fragments ('sentences') at the <br /> marks for you\nsentences = newdat['review'].apply(lambda x: x.split(\"<br />\")).values\nsentences = chain(*sentences)\n\n# Now split the sentences/fragments into word lists \n# YOUR CODE (1 to 4 lines of code):\ntrain_sentences = [sentence.split(\" \") for sentence in sentences]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:46:50.717453Z","iopub.execute_input":"2024-11-22T12:46:50.717892Z","iopub.status.idle":"2024-11-22T12:46:53.176813Z","shell.execute_reply.started":"2024-11-22T12:46:50.717848Z","shell.execute_reply":"2024-11-22T12:46:53.175796Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Check your work (Run this to get points!)\nstep_1.check()\n\n# You can ask for a hint or the solution by uncommenting the following:\n#step_1.hint()\nstep_1.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:46:53.179490Z","iopub.execute_input":"2024-11-22T12:46:53.179836Z","iopub.status.idle":"2024-11-22T12:46:53.193296Z","shell.execute_reply.started":"2024-11-22T12:46:53.179797Z","shell.execute_reply":"2024-11-22T12:46:53.192060Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.16666666666666666, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_CreateTextCatModel\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"questionId\": \"2_CreateTextCatModel\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n# A three-lines solution:\ntrain_sentences = []\nfor sentence in sentences:\n   train_sentences.append(sentence.split(' '))\n\n# A one-line solution (in case you're wondering):\ntrain_sentences = list(map(lambda x: x.split(' '), sentences))\n\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n# A three-lines solution:\ntrain_sentences = []\nfor sentence in sentences:\n   train_sentences.append(sentence.split(' '))\n\n# A one-line solution (in case you're wondering):\ntrain_sentences = list(map(lambda x: x.split(' '), sentences))\n\n```"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"What we don't have to do is encode the words as integers; `gensim` will take care of that for us. What we do need to do is provide `gensim` with a function with which it can decide whether to keep a word or discard a word from the vocabulary","metadata":{}},{"cell_type":"code","source":"words = []\nfor sentence in train_sentences:\n    words += sentence\nvocab = set(words)\n\nvocab = {word:0 for word in vocab}\nfor word in words:\n    vocab[word] += 1\n    \nvocab = sorted(vocab.items(), key = lambda x: x[1], reverse=True)[:1000]\nvocab = dict(vocab)\n\ndef trim_rule(word, count, min_count):\n    # ignore args 'count' and 'min_count'\n    from gensim import utils\n    return utils.RULE_KEEP if word in vocab else utils.RULE_DISCARD\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:46:53.194771Z","iopub.execute_input":"2024-11-22T12:46:53.195165Z","iopub.status.idle":"2024-11-22T12:46:58.368869Z","shell.execute_reply.started":"2024-11-22T12:46:53.195134Z","shell.execute_reply":"2024-11-22T12:46:58.367835Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Step 2: Learning word embeddings with `gensim`\n\nWe start with importing the `Word2Vec` module from the `gensim` library:","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:46:58.370204Z","iopub.execute_input":"2024-11-22T12:46:58.370568Z","iopub.status.idle":"2024-11-22T12:46:58.620187Z","shell.execute_reply.started":"2024-11-22T12:46:58.370534Z","shell.execute_reply":"2024-11-22T12:46:58.619148Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"The `Word2Vec` needs as inputs\n\n- The `sentences` broken up in tokens\n- The embedding dimension (`vector_size`); we'll use 50 as in the tutorial\n- A choice of architecture (0 for CBOW, 1 for Skip-gram); here we'll use CBOW\n- A window size that defines how many words around the target word will be used as _context_; we'll use `window = 3` as we did in the tutorial\n- Our rule for deciding whether to keep a word/token or discard it ([`trim_rule`](https://radimrehurek.com/gensim/models/word2vec.html))\n\nOther than that, we can specify the maximum number of tokens in the vocabulary (`max_vocab_size`), the number of training `epochs` (which we set to 10). We can also specify the number of CPU cores that we want to use (`workers`); we have 4 so why not use them? And we can specify that the vocabulary should be sorted by descending frequency before assigning a numeric ID.","metadata":{}},{"cell_type":"code","source":"%%time\n\nword2vec_model = Word2Vec(\n    sentences = train_sentences, \n    sg = 0,\n    vector_size = 50,\n    window = 3,\n    workers = 4,\n    sorted_vocab = 1,\n    epochs = 10,\n    trim_rule = trim_rule,\n    seed=101\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:46:58.621555Z","iopub.execute_input":"2024-11-22T12:46:58.621892Z","iopub.status.idle":"2024-11-22T12:47:58.871969Z","shell.execute_reply.started":"2024-11-22T12:46:58.621860Z","shell.execute_reply":"2024-11-22T12:47:58.870757Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3min, sys: 1.31 s, total: 3min 1s\nWall time: 1min\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"The \"cell magic\" command `%%time` at the start of the cell is a feature from Jupyter that allows to quickly assess how long the excution of the code inside the cell takes. Here you can see that `Word2Vec` is very fast (at least, compared to the training of the `embedding_learner` from the tutorial.\n\nThe information about the word embeddings, including the vocabulary and the word-numeric ID mappings is stored in the `wv` (word vectors) component of the model. \n\n#### Analogical reasoning with word embeddings\n\nThe `wv` object also has a number of methods that can perform various word similarity tasks to evaluate the quality of the word vectors. For instance, the analogy\n\n> Germany : Berlin ~ France : Paris\n\ncan be solved in terms of word embeddings by looking up the word embedding that is most similar to $\\mathbf{w}_\\text{germany} - \\mathbf{w}_\\text{berlin} + \\mathbf{w}_\\text{paris}$. Now, 'berlin', 'france', and 'paris' are not in our `vocab`ulary, but the similar analogy\n\n> director : script ~ actor : role\n\nconsists of words in our vocabulary. \n\nThis analogy can be solved with the `most_similar` method of the `word2vec_model.wv` object: It takes a list of words with a positive sign and a list of words with a negative sign, as in the expression $\\mathbf{w}_\\text{germany} - \\mathbf{w}_\\text{berlin} + \\mathbf{w}_\\text{paris}$:","metadata":{}},{"cell_type":"code","source":"# director : script ~ actor : role\nword2vec_model.wv.most_similar(positive=['director', 'role'], negative=['script'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:47:58.873425Z","iopub.execute_input":"2024-11-22T12:47:58.873819Z","iopub.status.idle":"2024-11-22T12:47:58.887631Z","shell.execute_reply.started":"2024-11-22T12:47:58.873784Z","shell.execute_reply":"2024-11-22T12:47:58.885295Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[('career', 0.6891260147094727),\n ('lee', 0.6317877769470215),\n ('mr_', 0.6181329488754272),\n ('john', 0.6166422367095947),\n ('actor', 0.5996233224868774),\n ('de', 0.5967867374420166),\n ('star', 0.5815402269363403),\n ('performance', 0.571494460105896),\n ('george', 0.5684173107147217),\n ('paul', 0.5660404562950134)]"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"The correct words '_actor_' and '_actress_' are in the top 5 of most similar word embeddings!\n\nNow use this function to solve the analogy\n\n> man : boy ~ woman : girl\n\nand store the result in the variable `analogy_sol`:","metadata":{}},{"cell_type":"code","source":"# Solve the analogy above with the fitted word2vec_model\n\n# YOUR CODE (1 line of code)\nanalogy_sol = word2vec_model.wv.most_similar(positive=['man', 'woman'], negative=['boy'])\nprint(analogy_sol)\n\nstep_2.check()\n\n# You can ask for a hint or the solution by uncommenting the following:\n#step_2.hint()\n#step_2.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:47:58.888926Z","iopub.execute_input":"2024-11-22T12:47:58.889392Z","iopub.status.idle":"2024-11-22T12:47:58.907887Z","shell.execute_reply.started":"2024-11-22T12:47:58.889328Z","shell.execute_reply":"2024-11-22T12:47:58.906675Z"}},"outputs":[{"name":"stdout","text":"[('doctor', 0.7448320388793945), ('lady', 0.7261499166488647), ('child', 0.707827091217041), ('girl', 0.6732783317565918), ('killer', 0.6682304739952087), ('person', 0.6567999720573425), ('named', 0.6496866345405579), ('cop', 0.6482382416725159), ('herself', 0.6206503510475159), ('hero', 0.6115176677703857)]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.16666666666666666, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"1_EvaluateFeedbackFormApproach\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: \n\n \n```python\nword2vec_model.wv.most_similar(positive=['man', 'woman'], negative=['boy'])\n```","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> \n\n \n```python\nword2vec_model.wv.most_similar(positive=['man', 'woman'], negative=['boy'])\n```"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Next you're going to use your own word embeddings to train a sentiment classifier.\n\n# Step 3: Encoding the reviews\n\nFor the sentiment classification task you first need to encode the reviews. The sentences need to be split, and the words should be encoded with the `word2vec_model`'s vocabulary. It's vocabulary is available as a dictionary in the `word2vec_model.wv.key_to_index`. Complete the function below to encode the reviews using `word2vec_model.wv.key_to_index` to translate words to ID codes. Words not in the vocabulary should be encoded with 999:","metadata":{}},{"cell_type":"code","source":"# Complete the code for the function `review_encoder`\n\ndef review_encoder(review, max_length=256):\n    review = review.replace((\"<br />\"), \"\").split()[:max_length]\n    # YOUR CODE (1 to 3 lines of code):\n    enc = []\n    # print(word2vec_model.wv.key_to_index.keys())\n    for word in review:\n        if word in word2vec_model.wv.key_to_index.keys():\n            enc.append(word2vec_model.wv.key_to_index[word])\n        else:\n            enc.append(999)\n    enc = (enc + ([0]*max_length))[:max_length] # make all sequence lengths equal to max_length\n    return enc\n\n\nreview_encoder(newdat.review.values[1])[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:47:58.912674Z","iopub.execute_input":"2024-11-22T12:47:58.913228Z","iopub.status.idle":"2024-11-22T12:47:58.937422Z","shell.execute_reply.started":"2024-11-22T12:47:58.913175Z","shell.execute_reply":"2024-11-22T12:47:58.935168Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[4, 385, 124, 364, 1, 0, 999, 999, 8, 56]"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Check your work to get the points(!)\nstep_3.check()\n    \n# You can ask for a hint or the solution by uncommenting the following:\n#step_3.hint()\n#step_3.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:47:58.942479Z","iopub.execute_input":"2024-11-22T12:47:58.943159Z","iopub.status.idle":"2024-11-22T12:47:58.955621Z","shell.execute_reply.started":"2024-11-22T12:47:58.943106Z","shell.execute_reply":"2024-11-22T12:47:58.953785Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.16666666666666666, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"3_TrainFunction\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: \n\n \n```python\ndef review_encoder(review, max_length=256):\n    review = review.split()[:max_length]\n    w2i = word2vec_model.wv.key_to_index\n    UNK_ID = len(w2i)-1\n    enc = [w2i[word] if word in w2i else UNK_ID for word in review]\n    enc = (enc + ([0]*max_length))[:max_length] # make all sequence lengths equal to max_length\n    return enc\n\n```","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> \n\n \n```python\ndef review_encoder(review, max_length=256):\n    review = review.split()[:max_length]\n    w2i = word2vec_model.wv.key_to_index\n    UNK_ID = len(w2i)-1\n    enc = [w2i[word] if word in w2i else UNK_ID for word in review]\n    enc = (enc + ([0]*max_length))[:max_length] # make all sequence lengths equal to max_length\n    return enc\n\n```"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"Now we can apply the `review_encoder` on the reviews in `newdat`, and store the encoded reviews in the `numpy` arrays `xtrain` and `ytrain`, in the same way as in the tutorial:","metadata":{}},{"cell_type":"code","source":"newdat['encoded_review'] = newdat.review.apply(review_encoder)\nnewdat['encoded_sentiment'] = newdat.sentiment.apply(lambda x: 1 if x == 'positive' else 0)\nnewdat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:47:58.958121Z","iopub.execute_input":"2024-11-22T12:47:58.959608Z","iopub.status.idle":"2024-11-22T12:48:04.756037Z","shell.execute_reply.started":"2024-11-22T12:47:58.959560Z","shell.execute_reply":"2024-11-22T12:48:04.754888Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment  \\\n0      one of the other reviewers has mentioned that ...  positive   \n1      a wonderful little production . <br /><br />th...  positive   \n2      i thought this was a wonderful way to spend ti...  positive   \n3      basically there _s a family where a little boy...  negative   \n4      petter mattei _s love in the time of money is ...  positive   \n...                                                  ...       ...   \n49995  i thought this movie did a down right good job...  positive   \n49996  bad plot , bad dialogue , bad acting , idiotic...  negative   \n49997  i am a catholic taught in parochial elementary...  negative   \n49998  i m going to have to disagree with the previou...  negative   \n49999  no one expects the star trek movies to be high...  negative   \n\n                                          encoded_review  encoded_sentiment  \n0      [30, 6, 0, 85, 999, 48, 999, 13, 106, 151, 42,...                  1  \n1      [4, 385, 124, 364, 1, 0, 999, 999, 8, 56, 999,...                  1  \n2      [11, 199, 12, 16, 4, 385, 101, 7, 999, 63, 24,...                  1  \n3      [666, 40, 15, 4, 230, 118, 4, 124, 416, 999, 4...                  0  \n4      [999, 999, 15, 117, 10, 0, 63, 6, 290, 8, 4, 9...                  1  \n...                                                  ...                ...  \n49995  [11, 199, 12, 20, 76, 4, 187, 205, 52, 291, 1,...                  1  \n49996  [88, 120, 2, 88, 407, 2, 88, 119, 2, 999, 957,...                  0  \n49997  [11, 232, 4, 999, 999, 10, 999, 999, 999, 34, ...                  0  \n49998  [11, 146, 167, 7, 28, 7, 999, 18, 0, 894, 913,...                  0  \n49999  [61, 30, 999, 0, 337, 999, 105, 7, 29, 324, 52...                  0  \n\n[50000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>encoded_review</th>\n      <th>encoded_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n      <td>[30, 6, 0, 85, 999, 48, 999, 13, 106, 151, 42,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a wonderful little production . &lt;br /&gt;&lt;br /&gt;th...</td>\n      <td>positive</td>\n      <td>[4, 385, 124, 364, 1, 0, 999, 999, 8, 56, 999,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n      <td>[11, 199, 12, 16, 4, 385, 101, 7, 999, 63, 24,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically there _s a family where a little boy...</td>\n      <td>negative</td>\n      <td>[666, 40, 15, 4, 230, 118, 4, 124, 416, 999, 4...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei _s love in the time of money is ...</td>\n      <td>positive</td>\n      <td>[999, 999, 15, 117, 10, 0, 63, 6, 290, 8, 4, 9...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>i thought this movie did a down right good job...</td>\n      <td>positive</td>\n      <td>[11, 199, 12, 20, 76, 4, 187, 205, 52, 291, 1,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>bad plot , bad dialogue , bad acting , idiotic...</td>\n      <td>negative</td>\n      <td>[88, 120, 2, 88, 407, 2, 88, 119, 2, 999, 957,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>i am a catholic taught in parochial elementary...</td>\n      <td>negative</td>\n      <td>[11, 232, 4, 999, 999, 10, 999, 999, 999, 34, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>i m going to have to disagree with the previou...</td>\n      <td>negative</td>\n      <td>[11, 146, 167, 7, 28, 7, 999, 18, 0, 894, 913,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>no one expects the star trek movies to be high...</td>\n      <td>negative</td>\n      <td>[61, 30, 999, 0, 337, 999, 105, 7, 29, 324, 52...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"xtrain = np.array(list(newdat.encoded_review.values))\nytrain = np.array(list(newdat.encoded_sentiment.values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:48:04.757486Z","iopub.execute_input":"2024-11-22T12:48:04.758398Z","iopub.status.idle":"2024-11-22T12:48:05.760859Z","shell.execute_reply.started":"2024-11-22T12:48:04.758345Z","shell.execute_reply":"2024-11-22T12:48:05.759644Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Step 4: Setting up the sentiment classifier\n\nWith the training data in the write format for Tensorflow, it's time to set up a sentiment classifier model. ","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:48:05.762196Z","iopub.execute_input":"2024-11-22T12:48:05.762558Z","iopub.status.idle":"2024-11-22T12:48:05.779765Z","shell.execute_reply.started":"2024-11-22T12:48:05.762517Z","shell.execute_reply":"2024-11-22T12:48:05.778590Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Make sure the model is properly set up and initialized. The word embeddings from `word2vec_model` are already extracted for you and stored in the `numpy` array `embeddings`. Also, most of the network is specified for you, including a `GRU` recurrent neural network layer for better performance:","metadata":{}},{"cell_type":"code","source":"# Complete the specification of the Keras model\n\n# YOUR CODE (~ 5 lines of code to complete)\nembeddings = word2vec_model.wv[vocab.keys()]\nvocab_size = embeddings.shape[0]\nembed_dim = embeddings.shape[1]\nweights_initializer = keras.initializers.Constant(embeddings)\n\nsentiment_classifier = keras.models.Sequential([\n    keras.layers.Embedding(\n        vocab_size, \n        embed_dim, \n        embeddings_initializer = weights_initializer, \n        trainable=False),\n    keras.layers.GRU(embed_dim, activation='relu', return_sequences=True),\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:48:05.781314Z","iopub.execute_input":"2024-11-22T12:48:05.782425Z","iopub.status.idle":"2024-11-22T12:48:07.155986Z","shell.execute_reply.started":"2024-11-22T12:48:05.782373Z","shell.execute_reply":"2024-11-22T12:48:07.154779Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Check your work\nstep_4.check()\n\n# You can ask for a hint or the solution by uncommenting the following:\n#step_4.hint()\n#step_4.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:48:07.157346Z","iopub.execute_input":"2024-11-22T12:48:07.157739Z","iopub.status.idle":"2024-11-22T12:48:07.170376Z","shell.execute_reply.started":"2024-11-22T12:48:07.157706Z","shell.execute_reply":"2024-11-22T12:48:07.169284Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.16666666666666666, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"4_PredictFunction\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: \n\n \n```python\n\n    embeddings = word2vec_model.wv[vocab.keys()]\n    weight_initializer = keras.initializers.Constant(embeddings)\n    inp_dim, out_dim = embeddings.shape\n\n    sentiment_classifier = keras.models.Sequential([\n        keras.layers.Embedding(output_dim=out_dim, input_dim=inp_dim, embeddings_initializer=weight_initializer, trainable=False),\n        keras.layers.GRU(50, activation='relu', return_sequences=True),\n        keras.layers.GlobalAveragePooling1D(),\n        keras.layers.Dense(1, activation = 'sigmoid')\n    ])\n    \n```","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> \n\n \n```python\n\n    embeddings = word2vec_model.wv[vocab.keys()]\n    weight_initializer = keras.initializers.Constant(embeddings)\n    inp_dim, out_dim = embeddings.shape\n\n    sentiment_classifier = keras.models.Sequential([\n        keras.layers.Embedding(output_dim=out_dim, input_dim=inp_dim, embeddings_initializer=weight_initializer, trainable=False),\n        keras.layers.GRU(50, activation='relu', return_sequences=True),\n        keras.layers.GlobalAveragePooling1D(),\n        keras.layers.Dense(1, activation = 'sigmoid')\n    ])\n    \n```"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Step 5: Train the sentiment classifier\n\nTrain the sentiment classifier. Train for 2 epoch's at a learning rate of 0.01 and 4 epochs at learning rate 0.0001. Use a batch size of 128, and a training-validation split of 80%-20%.","metadata":{}},{"cell_type":"code","source":"# Train the sentiment classifier\n\n# YOUR CODE (3 to 4 lines of code)\nsentiment_classifier.compile(keras.optimizers.Adam(0.01),'binary_crossentropy', metrics=['accuracy'])\nh = sentiment_classifier.fit(\n    xtrain, ytrain,\n    batch_size=32*4,\n    epochs=2,\n    validation_split=.2\n)\npd.DataFrame(h.history)[['loss','val_loss']].plot()\n\nsentiment_classifier.compile(keras.optimizers.Adam(0.0001),'binary_crossentropy', metrics=['accuracy'])\nh = sentiment_classifier.fit(\n    xtrain, ytrain, epochs=4,\n    batch_size=128,\n    validation_split=0.2\n)\npd.DataFrame(h.history)[['loss','val_loss']].plot()\n\n# Check your work\nstep_5.check()\n\n# You can ask for a hint or the solution by uncommenting the following:\n#step_5.hint()\n# step_5.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:48:07.171905Z","iopub.execute_input":"2024-11-22T12:48:07.172306Z","iopub.status.idle":"2024-11-22T12:57:22.899487Z","shell.execute_reply.started":"2024-11-22T12:48:07.172271Z","shell.execute_reply":"2024-11-22T12:57:22.898317Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\n313/313 [==============================] - 94s 295ms/step - loss: 0.4163 - accuracy: 0.8040 - val_loss: 0.3811 - val_accuracy: 0.8263\nEpoch 2/2\n313/313 [==============================] - 92s 292ms/step - loss: 0.3526 - accuracy: 0.8425 - val_loss: 0.3745 - val_accuracy: 0.8291\nEpoch 1/4\n313/313 [==============================] - 94s 294ms/step - loss: 0.3183 - accuracy: 0.8621 - val_loss: 0.3426 - val_accuracy: 0.8481\nEpoch 2/4\n313/313 [==============================] - 93s 296ms/step - loss: 0.3082 - accuracy: 0.8653 - val_loss: 0.3412 - val_accuracy: 0.8488\nEpoch 3/4\n313/313 [==============================] - 91s 290ms/step - loss: 0.3035 - accuracy: 0.8673 - val_loss: 0.3403 - val_accuracy: 0.8505\nEpoch 4/4\n313/313 [==============================] - 92s 293ms/step - loss: 0.3001 - accuracy: 0.8689 - val_loss: 0.3397 - val_accuracy: 0.8506\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.16666666666666666, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"5_EvaluateFunction\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: \n\n \n```python\n\n        # Training 2 epochs with learning rate 0.01:\n        sentiment_classifier.compile(keras.optimizers.Adam(1e-2),'binary_crossentropy', metrics=['acc'])\n        h = sentiment_classifier.fit(xtrain,ytrain, batch_size=32*4, epochs=2, validation_split=.2)\n\n        # Training 4 epochs with learning rate 0.0001:\n        sentiment_classifier.compile(keras.optimizers.Adam(1e-4),'binary_crossentropy', metrics=['acc'])\n        h = sentiment_classifier.fit(xtrain,ytrain, batch_size=32*4, epochs=4, validation_split=.2)\n        \n        # Visualize loss and validation loss over epochs\n        pd.DataFrame(h.history)[['acc','val_acc']].plot()\n        \n```","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> \n\n \n```python\n\n        # Training 2 epochs with learning rate 0.01:\n        sentiment_classifier.compile(keras.optimizers.Adam(1e-2),'binary_crossentropy', metrics=['acc'])\n        h = sentiment_classifier.fit(xtrain,ytrain, batch_size=32*4, epochs=2, validation_split=.2)\n\n        # Training 4 epochs with learning rate 0.0001:\n        sentiment_classifier.compile(keras.optimizers.Adam(1e-4),'binary_crossentropy', metrics=['acc'])\n        h = sentiment_classifier.fit(xtrain,ytrain, batch_size=32*4, epochs=4, validation_split=.2)\n        \n        # Visualize loss and validation loss over epochs\n        pd.DataFrame(h.history)[['acc','val_acc']].plot()\n        \n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":17},{"cell_type":"markdown","source":"# Step 6: Keep improving\n\nYou've built the necessary components to train a text classifier with using custom embeddings. What could you do further to optimize the model? How would you decide if the custom embeddings are even worth your while?\n\nRun the next line to check your answer.","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nstep_6.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:57:22.901184Z","iopub.execute_input":"2024-11-22T12:57:22.901660Z","iopub.status.idle":"2024-11-22T12:57:22.911025Z","shell.execute_reply.started":"2024-11-22T12:57:22.901613Z","shell.execute_reply":"2024-11-22T12:57:22.909855Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"6_ModelOptimizationQuestion\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n\nPossible avenues of improvement are increasing the embedding dimension (or decreasing embedding dimension of there is evidence of overtraining), considering different architectures, use more tokens from the reviews, increasing vocabulary size, trying a simple BoW model with conventional logistic regression, etc. Longer training of the embedding (i.e., more epochs) may also increase the quality of the embeddings and then, hopefully, improve performance. \n\nAre embeddings worth the effort? One reason to use embeddings is to reduce the input vector dimensions of text over one-hot encoding of all the words in the vocabulary. An benefit that arises from embeddings is that it makes the thought of the use of a recurrent neural layer (GRU) to contextualize individual embeddings more immediate: A sequence of vectors automatically suggests modeling the changes in the sequence as an (vector) auto-regression. However, all of that doesn't really logically require that the embedding vectors encode the correlation in use of the tokens. We could have met those objectives with randomly picked vectors as well.  \n\nIn fact when you replace the custom embeddings obtained with `gensim`'s `Word2Vec` with random vectors but use the same neural architecture you will see sentiment classification performance that is quite close! The architecture therefore appears to be more important than the embeddings—at least for sentiment classification of IMDB movie reviews.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n\nPossible avenues of improvement are increasing the embedding dimension (or decreasing embedding dimension of there is evidence of overtraining), considering different architectures, use more tokens from the reviews, increasing vocabulary size, trying a simple BoW model with conventional logistic regression, etc. Longer training of the embedding (i.e., more epochs) may also increase the quality of the embeddings and then, hopefully, improve performance. \n\nAre embeddings worth the effort? One reason to use embeddings is to reduce the input vector dimensions of text over one-hot encoding of all the words in the vocabulary. An benefit that arises from embeddings is that it makes the thought of the use of a recurrent neural layer (GRU) to contextualize individual embeddings more immediate: A sequence of vectors automatically suggests modeling the changes in the sequence as an (vector) auto-regression. However, all of that doesn't really logically require that the embedding vectors encode the correlation in use of the tokens. We could have met those objectives with randomly picked vectors as well.  \n\nIn fact when you replace the custom embeddings obtained with `gensim`'s `Word2Vec` with random vectors but use the same neural architecture you will see sentiment classification performance that is quite close! The architecture therefore appears to be more important than the embeddings—at least for sentiment classification of IMDB movie reviews.\n"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Keep Going\n\nThe next step is to dig a little deeper into [recurrent neural network layers](https://www.kaggle.com/datasniffer/nlp-recurrent-networks-for-entailment/).","metadata":{}}]}